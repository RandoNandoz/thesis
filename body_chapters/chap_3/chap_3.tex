\chapter{Evaluation}

\section{Evaluation Dataset}

\subsection{Criteria}

Since unit tests are used to catch defects in software, it's natural to use software defects to evaluate ExploTest. Therefore, we collected 22 bugs across 12 repositories.

In selecting bugs, we had a few criteria inspired by Pythoscope's \cite{wiatkowski2011pythoscope} creator:

\begin{enumerate}
  \item Since Python is a popular and general purpose programming language, we want bugs from various types of Python software.
  \item Due to design limitations associated with ExploTest, the program with the bug must support Python 3.13.7.
  \item Each bug must be easy to reproduce with an associated system test.
  \item Each bug must have an associated function that causes the bug --- i.e., the bug must be detectable by unit testing a function.
  \item The root cause of the bug must be from the source code itself, not due to platform issues (e.g., a bug in the interpreter or operating system).
  \item The bug cannot be so severe that the program does not execute or cause other processes to crash.
\end{enumerate}

\label{sec:Dataset}
\subsection{Dataset}
We evaluate ExploTest on 22 individual bugs across 12 repositories:

\begin{table}[H]
  \centering
  \caption{Repositories used in the evaluation dataset}
  \label{tab:eval_repositories}
  \begin{tabular}{@{} l l p{8.5cm} @{}}
    \toprule
    \textbf{Project name} & \textbf{Category}    & \textbf{Description}                                                                     \\ \midrule
    \verb|gunicorn|       & Web framework        & HTTP server.                                                                             \\
    \verb|click|          & UI library           & Library for creating command-line interfaces.                                            \\
    \verb|seaborn|        & Data visualization   & Data visualization library.                                                              \\
    \verb|Pygments|       & Source code analysis & Tool for syntax highlighting source code.                                                \\
    \verb|pylint|         & Source code analysis & Linter for Python code.                                                                  \\
    \verb|IPython|        & Meta tooling         & Enhanced interactive Python REPL.                                                        \\
    \verb|httpie|         & Command-line utility & Command-line HTTP client and API testing tool.                                           \\
    \verb|Tornado|        & Web framework        & Web framework.                                                                           \\
    \verb|black|          & Source code analysis & Python source code formatter.                                                            \\
    \verb|sanic|          & Web framework        & Web framework.                                                                           \\
    \verb|PySnooper|      & Meta tooling         & Debugging tool that traces function inputs, outputs, and state.                          \\
    \verb|youtube-dl|     & Command-line utility & Command-line tool for downloading videos from online streaming services (e.g., YouTube). \\
    \bottomrule
  \end{tabular}
\end{table}

Bugs were selected from the popular BugsInPy bug repository \cite{widyasari2020bugsinpy} and from selected repositories in QuAC \cite{wu2024quac}, a Python type inference tool.

Each bug consists of three things: the buggy commit, the fixed commit, and a bug report containing a system-test procedure on how to reproduce the bug.

\section{Environment}

The evaluation takes place on an x86\_64 computer running Ubuntu 24.04.3. All code is run using Python 3.13.7 in a virtual environment for each bug.

\section{Answering the RQs}

\subsection{ExploTest Outcomes}

The unit tests that ExploTest produces can be of different qualities, or it may not produce a unit test at all due to technical limitations. As discussed in the introduction,
unit tests have to exercise the function-under-test (FUT) and then assess its return value with an oracle, which in Python is an assertion. The table below outlines how we will categorize these different outcomes. In answering RQ2, we can group tests that

\subsection{Flakiness}
A unit test is considered flaky when it exhibits both passing and failing results on the same version of the source code without any changes to the code itself. We run the test twice as a rudimentary test for flakiness.

\begin{table}[p]
  \centering
  \caption{Per-bug evaluation results}
  \label{tab:per_bug_results}
  \small
  % Note: Changed width from \textheight to \linewidth for landscape mode
  \begin{tabularx}{\linewidth}{@{} l l r Y Y Y Y Y @{}}
    \toprule
    \textbf{Repo}                &
    \textbf{ID}                  &
    \textbf{\# of tests}         &
    \textbf{Function called}     &
    \textbf{Output correct}      &
    \textbf{Oracle created}      &
    \textbf{Passes on good code} &
    \textbf{Fails on bad code}                                                       \\
    \midrule
    black                        & 12    & 490         & Yes & Yes & No  & N/A & N/A \\
    black                        & 13    & 3           & Yes & Yes & Yes & Yes & No  \\
    black                        & 15    & 1           & Yes & Yes & Yes & Yes & No  \\
    httpie                       & 3     & 1           & Yes & Yes & Yes & Yes & Yes \\
    PySnooper                    & 2     & 4           & Yes & Yes & Yes & Yes & No  \\
    PySnooper                    & 3     & 1           & Yes & Yes & Yes & Yes & No  \\
    sanic                        & 1     & 6           & Yes & Yes & Yes & Yes & No  \\
    sanic                        & 2     & 0           & N/A & N/A & N/A & N/A & N/A \\
    sanic                        & 3     & 3           & Yes & Yes & Yes & Yes & Yes \\
    tornado                      & 10    & 0           & N/A & N/A & N/A & N/A & N/A \\
    tornado                      & 14    & 2           & Yes & Yes & No  & N/A & N/A \\
    tornado                      & 15    & 1           & Yes & Yes & Yes & Yes & No  \\
    tornado                      & 4     & 2           & Yes & Yes & Yes & Yes & No  \\
    youtube-dl                   & 12    & 0           & N/A & N/A & N/A & N/A & N/A \\
    click                        & 3071  & 4 P / 4 ARR & Yes & Yes & No  & Yes & No  \\
    gunicorn                     & 3144  & 0           & N/A & N/A & N/A & N/A & N/A \\
    ipython                      & 14930 & 0           & N/A & N/A & N/A & N/A & N/A \\
    pylint                       & 10373 & 1           & Yes & Yes & Yes & Yes & No  \\
    pylint                       & 10402 & 2           & Yes & Yes & Yes & Yes & No  \\
    seaborn                      & 3542  & 1           & Yes & Yes & No  & Yes & No  \\
    seaborn                      & 3553  & 0           & N/A & N/A & N/A & N/A & N/A \\
    pygments                     & 2382  & 1           & Yes & Yes & Yes & Yes & No  \\
    \bottomrule
  \end{tabularx}
\end{table}
% \end{landscape}

\subsubsection{Table columns}
\begin{itemize}
  \item \textbf{Repo}: The repository the bug originates from.
  \item \textbf{ID}: For bugs from BugsInPy, the BugsInPy ID that the bug is associated with. For bugs from QuAC, this is the GitHub issue number that the bug is associated with.
  \item \textbf{\# of tests}: The number of unit tests generated by the tool. The tool can generate $>1$ test when the FUT is exercised more than once by the system test, or if recursive calls occur. $0$ tests are generated when the tool fails to generate any test at all.
  \item \textbf{Function called}: Does the tool correctly setup the required
  \item \textbf{Output correct}: Whether the output of the function-under-test exercised by the unit test is equivalent to the system test.
  \item \textbf{Oracle created}: Whether assertions were created to determine the software functions correctly.
  \item \textbf{Passes good code}: Whether the assertions pass when running on the correct code (i.e., the code the test was generated on).
  \item \textbf{Fails on bad code}: Whether the assertions fail when running on the buggy code (i.e., they detect the bug).
\end{itemize}

% flatten structures
%
\subsection{Preliminary Evaluation}

\subsubsection{Preparation}
For each bug, we will create a new Python virtual environment using the \verb|venv| module.

We then install the package and its dependencies as instructed by the documentation of the package, along with ExploTest. For seaborn, since the bugs appear as plots, we install \verb|jupyter| and \verb|notebook| to work with plots interactively.


\subsubsection{Finding carve target}
% To evaluate RQ1, we will put the \verb|@explore| at the first public method that exposes the bug. PEP8, the Python style guide Public methods are identified as non-closure functions not starting with an underscore (\verb|_|).

ExploTest uses an \verb|@explore| decorator to mark functions to carve in the source code. By placing the decorator, the function is transformed to first perform a static analysis of the function's source code (e.g., to get the function name and parameter/package information).

To evaluate RQ1, we can split the dataset into two by bug origin: bugs from BugsInPy and bugs from QuAC. Bugs from BugsInPy come with buggy functions already known, while the causes of bugs from QuAC are indeterminate. While the bugs from QuAC may contain fixes in pull requests that identify the functions that introduce the bug, we refrain from looking at them to help answer RQ5 with a cleaner slate.

\subsubsection{Locating carve targets for bugs from BugsInPy}
The data provided by BugsInPy already identify the individual lines that cause the bug to occur.

For us, the first thing to do is to place the \verb|@explore| at the nearest function to the lines that cause the bug to occur.
However, these functions might not be good candidates for carving for two reasons: (1) parameters might not be serializable, as discussed in chapter 2, or (2), the return value of the function will not allow us to generate an oracle that detects the bug.

Oftentimes, human inspection of the source code allows us to determine the presence of non-serializable objects. We can infer these properties from variable names, for example, variables named \verb|frame| suggest the presence of frame objects which contain pointers to native code that cannot be serialized.

A similar manual source code analysis works to identify return values not conducive to good test oracles. We can inspect the source code for free-standing \verb|return|s that indicate a function that is used for its side effects only.

When we identify a method that is a bad candidate for test carving, we  walk up the call graph of the function (i.e., look at the caller and the usage of our target function), and place an \verb|@explore| once we determined that the objects are serializable, or keep looking up.

There are cases where we cannot find any suitable candidates, in which case we consider the unit test to not be carvable with our tool.

\subsubsection{Locating carve targets for bugs from QuAC}



\subsubsection{Answering RQ1}
\begin{enumerate}
  \item Run the non-buggy version of the code and function and observe its output.

  \item Use the procedure outlined above to place the \verb|@explore| to mark the function-under-test for carving.

  \item Generate the test on the non-buggy version of the code using the system test case:

        Did the tool crash? Were exceptions or errors emitted and what kind?

  \item If a non-empty file was produced, inspect the file and run the test using a debugger to determine whether the FUT was reached. Observe the output of the function from the generated the unit test against the output of the system test. If they are the same, we can consider the FUT to be exercised.

  \item When running the test, we can observe whether the test passes on the non-buggy code. If it passes, we move and apply the buggy commits. If the test fails on the source it was generated on, we note it down and skip the next step.

  \item We then run the test, generated on the non-buggy code on the buggy version of the codebase. The test should not pass, and if it does, we consider the oracle defective.
\end{enumerate}

This procedure should allow us to bucket each created test by the table, and facilitate discussion of failure cases.

\subsubsection{Answering RQ2}
Using the buckets we have in \hyperref[sec:Dataset]{section 3.1.2}, we can summarize functioning unit tests (what do we consider functioning?) by the program family they belong to. We will discuss characteristics that make programs conducive to unit test carving.

\subsubsection{Answering RQ3}
We time the time it takes to execute the system test with GNU \verb|time| version 1.9 on our environment mentioned in section 3.2. We calculate and discuss three metrics:

\begin{itemize}
  \item The bug detection speedup, $\frac{T^{B}_{\text{sys}}}{T^{B}_{\text{unit}}}$, which is the speedup multiplier offered by the unit test to catch buggy code.

        It is the ratio of the time it takes for a system test on buggy code to run ($T^{B}_{\text{sys}}$) and the time it takes for a unit test on buggy code ($T^{B}_{\text{unit}}$).
  \item The regression verification speedup, $\frac{T^{P}_{\text{sys}}}{T^{P}_{\text{unit}}}$, which is the speedup multiplier offered by the unit tests to validate code changes on a correct codebase.

        It is the ratio of the time it takes for a system test on correct code to run ($T^{P}_{\text{sys}}$) and the time it takes for a unit test on correct code ($T^{P}_{\text{unit}}$).

        % q for next meeting: is this meaningful? considering cache warm ups, etc.
  \item Carving time, which is the difference between the system test running with ExploTest and without ExploTest.

        It is the ratio of the time to run the program while carving and without carving.
\end{itemize}

\subsubsection{Answering RQ4}
When carving tests, the most common cause of failure are non-serializable objects. We'll discuss the root cause of these objects and their frequencies. Another failure mode is a defective
oracle that either fails on correct code, or oracles that pass on incorrect code.

\subsubsection{Answering RQ5}
For the assessment of this RQ to remain valid, we purposely discard the bugs found from BugsInPy, since the author was already exposed to the root causes of these bugs.
Instead, the author will try to debug and root cause analyze the bugs from QuAC, using ExploTest, and determine if there is a procedure to using ExploTest to find, and potentially patch bugs.

% \subsubsection{To answer RQ1 I will...}k

% \subsubsection{To answer RQ2 I will...}
% \subsubsection{To answer RQ3 I will...}
% \subsubsection{To answer RQ4 I will...}

% Tables can be placed with the \textbf{\textbackslash table} environment. The same instructions for the figures apply here; use the \textbf{\textbackslash label} command often

% \begin{center}
%   \begin{table}[h]
%     \centering
%     \begin{tabular}{l c l c}
%     \hline
%     Layer Type & Size & Activation & Regularization\\
%     \hline
%     Flatten& $30 \times 30 \times 3$ &ReLU & ---\\
%     Dense & 4096 & ReLU & 10\% Dropout\\
%     Dense & 2048 & ReLU & 10\% Dropout\\
%     Dense & 1024 & ReLU & 10\% Dropout\\
%     Dense & 1024 & ReLU & --- \\
%     Dense & 6 & Softmax & --- \\
%     \hline
%     \end{tabular}
%     \caption[Short Caption for List of Tables.]{Long caption to appear in the body of the text with the table.}
%     \label{tab:ModelArchitecture}
%   \end{table}
% \end{center}

% \section{Another Section}

% \section{Some Other Section}

% \subsection{A Subsection}
