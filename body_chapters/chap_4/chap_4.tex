\chapter{Results}
% This is a reference to a label in another chapter. Appendix~\ref{chap:app1}. You can always refer to other
% labels from other chapters with the
% \textbf{\textbackslash ref} command.

% \section{Common Failure Modalities in Existing Unit Test Carving Tools}
% ? Look at literature?

% \section{Common Failure Modalities of ExploTest}

\section{Answers to RQs}

\subsection{RQ1: Test quality}

In RQ1, we posed: \textit{To what extent does the unit test carving tool we created generate viable
    unit tests when applied to popular,
    open-source Python repositories?}


\hyperref[sec:RQ1Results]{Table C.1} shows ExploTest is able to generate at least 1 test for 16 curated bugs out of 22 bugs
from the software repositories we choose. We consider these runs successful. $72\%$ of ExploTest runs were able to create
some kind of syntactically valid unit test. The 6 unsuccessful runs are examined further in RQ4.

Out of the 16 runs that produced a unit test, all were able to correctly set up the arguments to the function
and successfully called the function.
$72\%$ of ExploTest runs create unit tests that when run, reach the function-under-test.

Manual analysis of the returned object's state showed that 14 out of 16 of the functions-under-test exercised by the unit test
had the same output as the function-under-test being exercised by the system test. The 2 cases of incorrect function object
were due to the custom serialization protocol a parameter used; ExploTest has no control over the customized serialization code
that implementors use.
$64\%$ of ExploTest runs create unit tests that reach and correctly call the function-under-test the same as a system test would.

AutoAssert was able to create assertions for 12 out of 14 of the tests that correctly call the functions-under-test.
$55\%$ of ExploTest runs create unit tests that correctly exercise the function-under-test and have an oracle.

11 out of 12 tests with assertions when run on correct code pass, and only 2 tests fail when run on incorrect code.
Only $9\%$ of the tests generated by ExploTest actually expose the bugs they intend to detect. Our approach
to generating assertions is likely too naive or conservative. Further discussion on improving the quality of
assertions will follow in section 5.

\noindent\fbox{%
    \parbox{\textwidth}{%
        RQ1 Answer:
        \textbf{64\% of ExploTest runs create unit tests that correctly exercise the behaviour of their carving targets.
            Only 9\% of runs create tests that expose buggy behaviour. Test quality degrades sharply at the assertion generation
            phase: only 2 of 14 correctly-excerising tests produce assertions that expose the target bug.}
    }%
}


\subsection{RQ2: Unit test viability by software attributes and family}

RQ2 poses the question: \textit{What software families and attributes are associated with the differing viabilities
    of unit tests created by the tool?}

Differing viabilities are classified as follows:

\begin{itemize}
    \item Defective --- the function was either not called, or called but produced a result that differs from the system test.
          There are 8 runs in this category.
    \item Exercises function --- the function was called by the unit test, and its output was the same as the system test.
          There are 14 runs in this category.
    \item Uncovers bug --- the unit test is able to exercise the function, and can uncover the bug in the buggy version of
          the repository with its oracle. This category is a subset of the ``exercises function'' category.
          There are 2 runs of the test in this category.
\end{itemize}

\subsubsection{Tests which can uncover bugs}
First, we discuss the excellent. What attributes of the function-under-test and codebase contribute are associated with
the tool creating a test that falls into the ``uncovers bug'' category?

Observe the test generated when run on the \verb|update_headers| procedure from \verb|httpie|
in \hyperref[sec:httpie-fully-functional-ut]{A.1}. The test is able to detect the bug since in
the buggy version the check for \verb|value is None| is not present and causes the reference
to \verb|value.decode| to throw a \verb|NameError|.

The other test for in the ``uncovers bug'' category for the \verb|sanic| web framework in \hyperref[sec:sanic-fully-functional-ut]{A.2}
has similar attributes to the first. The function-under-test, \verb|url_for|'s only external data dependencies are on the \verb|re|
regular expression library and the \verb|urlencode|, \verb|urldecode| functions from \verb|urllib| library which do not require
any previous state to be set up for them.

We note a few things from the two tests that uncover bugs:

\begin{itemize}
    \item The function-under-test has limited external dependencies for it to function.
    \item The function-under-test's external dependencies do not require their states to be set; they
          are stateless, or the default state is sufficient.
    \item Every type used in the function-under-test is serializable. The \verb|request_headers|
          parameter is a dictionary of strings and bytes, which are both serializable.
    \item Even with weak test oracles like ~\verb|assert return_value is None| can catch bugs, since
          crash oracles exist.
\end{itemize}

\subsubsection{Tests which can exercise their target function-under-tests}
We'll discuss common patterns in ExploTest runs that successfully create a unit test, but cannot
detect the bug.

Looking at our 14 runs in this category, we can make a few observations:

\begin{itemize}
    \item Target function-under-tests have serializable parameters. This is unsurprising, considering
          the tool uses record-and-replay techniques to carve its tests.
    \item Carve candidates have clearly defined entry-points.
\end{itemize}


\subsubsection{Defective tests}

\textbf{Defect: No test generated}

\textbf{Defect: Test generated, FUT not correctly exercised}


\subsubsection{Viability by software family}

We aggregate the ratio of successful tests by software category in \hyperref[sec:RQ2Results]{Table D.1}. The Data visualization,
Command line utility, and Web framework categories are discarded since there is not enough data to draw a meaningful analysis.

% tldr, lol
\subsubsection{RQ Answer}





\subsection{RQ3: \textit{What is the amount of speedup offered by the unit tests created by the tool compared to
        their system test runs?}}

\subsection{RQ4: \textit{What modes of failure occur when the tool fails to create any kind of unit test?}}

\subsection{RQ5: \textit{What is the procedure to use ExploTest to catch defects in software?}}


